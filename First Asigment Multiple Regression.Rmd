---
title: "Multiple Regression"
author: "Ignacio Almodovar & Andres Mejia"
date: "11/23/2021"
output:
  #pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(magrittr)
library(ggplot2)
setwd("Datasets_Chapters_1_to_4")
```

## 1. Show that the properties of least squares estimators are satisfied using the following definitions:

$$
\begin{matrix}
\hat{\beta}=(X'X)^{-1}X'Y\\
\hat Y=X(X'X)^{-1}X'Y=HY\\
\hat\epsilon=Y-\hat Y=(I-H)Y
\end{matrix}
$$

### The residuals are orthogonal to the predictors.
### The sum of the residuals is zero.

This property is equivalent to $x_i^t \hat \epsilon=0$ or in matrix form $X'\hat \epsilon =0$, this also includes the second result due to the fact that $X$ contains a column of ones in order to account for the intercept term. To see why this is true consider.

$$
X'\hat \epsilon=X'(I-H)Y=X'(I-X(X'X)^{-1}X')Y=(X'-X'X(X'X)^{-1}X')Y=(X'-X')Y=0
$$

### The sum of the observed data is equal to the sum of the fitted values:

To prove this we will use the following descomposition of the $Y$ vector.


$$
Y=HY+(I-H)Y=\hat Y +\hat \epsilon
$$
Then multiplying with a column of ones and using the ortogonality of the ones with the error term

$$
\begin{matrix}
1^tY=1^t\hat  Y +1^t \hat \epsilon\\
\sum_{i=1}^n Y_i= \sum_{i=1}^n \hat Y_i +0=\sum_{i=1}^n \hat Y_i
\end{matrix}
$$


### The residuals are orthogonal to the fitted values.

Let's calculate the product
 
 $$
( \hat Y )'\hat \epsilon =(HY)'(I-H)Y=Y'H(I-H)Y=Y'(H-H)Y=Y'0Y=0
 $$
Here we used the fact that $H$ is symmetric and idempotent ($H^2=H$).

# 2. Using model modall, check numerically that the properties of the least squares estimates are satisfied.

### The sum of the residuals is 0 

$$\sum^{n}_{i=1}=\hat{\epsilon_i}$$
```{r}
bodyfat <- read.table(file = "Datasets_Chapters_1_to_4/bodyfat.txt", header = TRUE)
modall <- lm(hwfat ~., data = bodyfat)
sum(modall$residuals)
```

### The sum of the observed data is equal to the sum of the fitted values

$$\sum^n_{i=1}Y_i=\sum^n_{i=1}\hat{Y}_i$$

```{r}
sum(bodyfat$hwfat)
sum(modall$fitted.values)
```

### The residuals are orthogonal to the preditors

$$\sum_{i=1}^n x_{i}\hat{\epsilon}_i$$

```{r}
bodymatrix=as.matrix(select(bodyfat,-hwfat))
sum(t(bodymatrix)%*%modall$residuals)
```


### The residuals are orthogonal to the fitted values

$$\sum_{i=1}^n \hat{y_{i}}\hat{\epsilon}_i$$

```{r}
sum(modall$fitted.values*modall$residuals)
```

# 3.Check that for the dataset index.txt, the least squares es- timates of the parameters are: βˆ0 = 4.267 and βˆ1 = 1.373, using the results in section 2.4.1 (not using the lm() function). 


$$\hat{\beta_0}=\overline Y - \hat{\beta_1}\overline X$$
$$\hat{\beta_1}=\frac{S_{xy}}{S^2_x}$$

Using R we can easily compute all those values.

```{r}
indice=read.table(file="Datasets_Chapters_1_to_4/index.txt", header = TRUE)
hatx=mean(indice$PovPct)
haty=mean(indice$Brth15to17)
varx=var(indice$PovPct)
samcov=cov(indice$PovPct,indice$Brth15to17)
b1=samcov/varx
b0=haty-b1*hatx
cat("b0=",b0,"b1=",b1)
```


#4  Given the response variable y and the covariates x2 and x3 in the dataset Transform V2.txt dataset , check if is necessary to transform any variable and the residual graphs to show that the transformed model is correct.

```{r,echo=false}
TransformV2=readr::read_table("Transform_V2.txt")
library(psych)
library(ggfortify)
```

First of all we plot a panel where we can see and compare the different distributions that our predictors follow, as well as the correlations between them. 

```{r}
pairs.panels(TransformV2, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
```

We can see that the response "y" is very correlated to both "x2" and "x3". Therefore, the response could be expressed by just one of the values. Nevertheless we are first going to fit a model using both predictors and see how it behavies.

```{r}
autoplot(lm(data=TransformV2,y~x2+x3))
```

In the first plot we can see that the variance between the Residuals and the Fitted values is not constant at all

In order to fix that, we can apply a transformation to our variables. First of all we are going to try to transform both of them using the square. 

```{r}
autoplot(lm(data=TransformV2,y~x2+x2^2+x3+x3^2))
```

As we have not obtained the results that we wanted we are going to try a different transformation.

As we saw before, in the first panel of plots we saw that the response Y was very correlated to both "x2" and "x3". Therefore, we are going to try a new model using just the square root transformation of just one of the variables.

```{r}
m4=lm(data=TransformV2,y~x3+I(x3^(1/2)))
summary(m4)
autoplot(m4)

# m6=lm(data=TransformV2,y~x3+I(x3^(2)))
# summary(m6)
# autoplot(m6)
# 
# m5=lm(data=TransformV2,y~x2+I(x2^2))
# summary(m5)
# autoplot(lm(data=TransformV2,y~x2+x2^2+x2^3+x3+x3^2+x3^3))
# autoplot(m5)
# 
# infa=influence.measures(m4)
# infa
# 
# sort(infa$infmat[,"cook.d"]4/)

```

Within this last transformation we can see that we have achieve the linear regression assumptions for a model, and we can say that it is good enough to be used. The variance is constant in all the trace of the plot, and it fits well the qnorm line. 

<!-- Bullet points -->

<!-- 1. Observamos que en las graficas de scale location y fitted tenemos una linea aproximadamente rectas con puntos distribuidos de forma homogenea tanto en la parte superior como inferior de la linea. -->

<!-- 2. Observamos que los residuales parecen distribuirse de manera normal viendo la gráfica qq. -->

<!-- 3. Ningun punto tiene una distacia de cook pronblematica, es decir, mayor a 0.5. -->


# 5

Again, for this exercise we are also going to plot a first panel to show how our data behaves.

```{r}
Transform2_V2=readr::read_table("Transform2_V2.txt")

pairs.panels(Transform2_V2, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )

modelo0=lm(data=Transform2_V2,y2~x1+x2)
autoplot(modelo0)
```
Again we can see that the model used does not fit the normality assumptions, so we are again going to transform the data. However, in this case, instead of transforming the predictors it is better to transform the response variable using the X method.


```{r}
MASS::boxcox(data=Transform2_V2,y2~x1+x2)
Transform2_V2 %<>% mutate(y2=(y2^-1-1)*-1)

pairs.panels(Transform2_V2, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
```
Once we have our response normalized, we can compute our new model using again just one of the predictors, as both of them are very correlated between them and with the response variable.

```{r}
m2=lm(data=Transform2_V2,y2~x1)
summary(m2)
autoplot(m2)
```

#6

We can write the bias of the ridge estimator as:

$$\hat\beta_\lambda=(X^tX+\lambda I)^{-1}X^{t}Y$$
As we know that $Y=(X\beta+\epsilon)$, then:

$$\hat\beta_\lambda=(X^tX+\lambda I)^{-1}X^{t}(X\beta+\epsilon)=(X^tX+\lambda I)^{-1}X^{t}X\beta + (X^tX+\lambda I)^{-1}X^t\epsilon$$

Therefore, the expected value of $\hat\beta_\lambda$:

$$E[\hat\beta_\lambda]=(X^tX+\lambda I)^{-1}X^{t}X\beta + (X^tX+\lambda I)^{-1}X^tE[\epsilon]\\=(X^tX+\lambda I)^{-1}X^{t}X\beta*0=\\ =(X^tX+\lambda I)^{-1}X^{t}X\beta$$
With this result we can say that the ridge estimator is unbiased if and only if $(X^tX+\lambda I)^{-1}X^{t}X=I$. This is only possible if $\lambda=0$ and that is when the ridge estimator coincides with the OLS estimator. Therefore, the bias of the estimator is:

$$E[\beta_{\lambda}]-\beta=  (X^tX+\lambda I)^{-1}X^{t}X\beta = (X^tX+\lambda I)^{-1}X^{t}X\beta-(X^tX)^{-1}X^tX\beta=\\=[(X^tX+\lambda I)^{-1}-(X^tX)^{-1}]X^tX\beta$$

To show that $Var[\hat\beta_{OLS}]\geq Var[\hat\beta_{ridge}]$:

We know that $Var[\hat\beta]=\sigma^2(X^tX)^{-1}$. Then,

$$
Var[\hat\beta_\lambda]=\sigma^2(X^tX+\lambda I)^{-1}X^tX(X^tX+\lambda I)^{-1}=\\=\sigma^2(X^tX+\lambda I)^{-1}X^tX(X^tX)^{-1}X^tX(X^tX+\lambda I)^{-1}
$$
Defining M as $M=X^tX(X^tX+\lambda I)^{-1}$, the difference of variances is:

$$
Var[\hat\beta_{OLS}] - Var[\hat\beta_{ridge}]  =  \sigma^2(X^tX)^{-1} - \sigma^2M^t(X^tX)^{-1}M=\\ =\sigma^2[M^t(M^t)^{-1}(X^tX)^{-1}M^{-1}M - M^t(X^tX)^{-1}M]=\\
=\sigma^2M^t[(M^t)^{-1}(X^tX)^{-1}M^{-1}-(X^tX)^{-1}]M=\\
=\sigma^2M^t[(X^tX)^{-1}(X^tX+\lambda)(X^tX)^{-1}(X^tX+\lambda)(X^tX)^{-1}-(X^tX)^{-1} ]M=\\
=\sigma^2M^t[(I+\lambda(X^tX)^{-1})(X^tX)^{-1}(I+\lambda(X^tX)^{-1})-(X^tX)^{-1}]M=\\
=\sigma^2M^t[((X^tX)^{-1}+\lambda(X^tX)^{-2})(I+\lambda(X^tX)^{-1})-(X^tX)^{-1}]M=\\
=\sigma^2 M^t[(X^tX)^{-1}+\lambda (X^tX)^{-2}+\lambda(X^tX)^{-2}+\lambda^2(X^tX)^{-3}-(X^tX)^{-1}]M=\\
=\sigma^2 M^t[2\lambda(X^tX)^{-2}+\lambda^2(X^tX)^{-3}]M=\\
=\sigma^2(X^tX+\lambda I)^{-1}X^tX[2\lambda(X^tX)^{-2}+\lambda^2(X^tX)^{-3}]X^tX(X^tX+\lambda I)^{-1}=\\=\sigma^2(X^tX+\lambda I)^{-1}[2\lambda I+\lambda^2(X^tX)^{-1}](X^tX+\lambda I)^{-1}
$$
If $\lambda>0$, the latter matrix is positive definite because for any $v\neq 0$, we have:

$$z=(X^tX+\lambda I)^{-1}v\neq0$$
Therefore, we finally have:

$$
v^T(Var[\hat\beta_{OLS}] - Var[\hat\beta_{ridge}])v=\sigma^2z^t(2\lambda I+\lambda^2(X^tX)^{-1})z=\sigma^2\lambda z^tz+\sigma^2\lambda^2x^t(X^tX)^{-1}z>0
$$
Because $X^tX$ and its inverse are positive definite.

#7

#8


```{r}
insurance=readr::read_csv("Datasets_Chapters_1_to_4/insurance.csv")
head(insurance)

pairs.panels(insurance %>% dplyr::select(y3,age2,charges), 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )

#normalize charges
w=MASS::boxcox(data=insurance,charges~.)
insurance %<>% mutate(y2=log(charges+1),y3=(charges^(0.101)-1)/0.101)

histogram(insurance$y2)
histogram(insurance$y3)
qplot(insurance$y2,geom = "density")
qplot(insurance$y3,geom = "density")
qqnorm(insurance$y2)
ggplot(data=insurance,aes(sample=y2)) + geom_qq() + geom_qq_line()

ggplot(data=insurance,aes(sample=y3))+ geom_qq_line() + geom_qq(color="red")

qqnorm(insurance$y3)

histogram(insurance$y3)
t=median(insurance$y3)
insurance$y4=insurance$y3-t
y43=LambertW::Gaussianize(insurance$y3)
histogram(y43)

qqnorm(y43)


w=BoxCox.lambda(insurance$age)
insurance %<>% mutate(age2=(charges^(w)-1)/w)
t22=BoxCox(insurance$age,w)
insurance$age2=t22
```

En principio nos quedamos con la tranformacion boxcox con lambda=0.101

```{r}
ggplot(data=insurance,aes(y=y3,x=age,color=smoker)) + geom_point()
names(insurance)
modelo_1=lm(data=insurance,y3~age+smoker+age:smoker)
summary(modelo_1)
autoplot(modelo_1)

insurance$r1=modelo_1$residuals
insurance$fit1=modelo_1$fitted.values

insurance$children %<>% as.factor()
insurance$bmi1=insurance$bmi %>% cut(c(0,18.5,25,30,100))
ggplot(data=insurance,aes(x=fit1,y=r1,color=region)) + geom_point()

```

```{r}
modelo_2=lm(data=insurance,y3~age+smoker+age:smoker+bmi+bmi:smoker+children)
summary(modelo_2)
autoplot(modelo_2)
```

```{r}
insurance$r1=modelo_2$residuals
insurance$fit1=modelo_2$fitted.values

ggplot(data=insurance,aes(x=age,y=region,color=groups)) + geom_boxplot()

ggplot(data=insurance,aes(x=sex,fill=children))+ geom_bar() + facet_grid(vars(groups))

insurance$groups=(insurance$fit1>18.3) | (insurance$r1>0.48)


```
```{r}
modelo_3=lm(data=insurance,y3~age+I(age^2)+smoker+age:smoker+bmi+bmi:smoker+children+smoker:children)
summary(modelo_3)
plot(modelo_3)

insurance$r1=modelo_3$residuals
insurance$fit1=modelo_3$fitted.values

ggplot(data=insurance,aes(x=fit1,y=r1,color= charges)) + geom_point()

insurance$groups2=(insurance$r1>0.1)

pairs.panels(insurance %>% dplyr::select(r1,bmi), 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )

ggplot(data=insurance,aes(x=age,y=,color= groups,shape=smoker)) + geom_point() #scale_color_continuous(type="viridis")

insurance$logbmi=log(insurance$bmi+1)


```
Volvemos a empezar!!!!!!!

```{r}
#dividimos data set en validacion y trainig
library(caret)
library(stringr)

insurance=readr::read_csv("Datasets_Chapters_1_to_4/insurance.csv")
insurance$children %<>% as.factor()

insurance$highrisk=insurance$bmi>=30 & insurance$smoker=="yes" 
insurance$north=str_detect(insurance$region,"north")


index=createDataPartition(insurance$charges,p=0.7)
trainset=insurance[index$Resample1,]
testset=insurance[-index$Resample1,]

index2=createDataPartition(testset$charges,p=0.66)
crossvalidation=testset[index2$Resample1,]
finalset=testset[-index2$Resample1,]




modelo_0_logico=lm(data=trainset,charges~age+bmi:smoker+sex+sex:children)
summary(modelo_0_logico)
plot(modelo_0_logico)

trainset$residuales=modelo_0_logico$residuals
trainset$fit1=modelo_0_logico$fitted.values

#Esta grafica incluirla
ggplot(trainset,aes(x=residuales,y=fit1,color=smoker)) + geom_point()
#ggplot(trainset,aes(x=residuales,y=fit1,color=highrisk)) + geom_point()
ggplot(trainset,aes(x=residuales,y=fit1,color=bmi)) + geom_point() + scale_color_continuous(type = "viridis")


##Ajustamos charges
t=MASS::boxcox(data=testset,charges~smoker:age+region+smoker+bmi)
indicemax=which.max(t$y)
lambda=t$x[indicemax]

trainset$y2=forecast::BoxCox(trainset$charges,lambda)
crossvalidation$y2=forecast::BoxCox(crossvalidation$charges,lambda)
finalset$y2=forecast::BoxCox(finalset$charges,lambda)

#Best model so far ####
modelo_1_ajustar_y=lm(data=trainset,y2~smoker:bmi+region+smoker+children+age)
summary(modelo_1_ajustar_y)
autoplot(modelo_1_ajustar_y)[[1]] + geom_point(aes(color=trainset$children))
plot(modelo_1_ajustar_y)
#######################

#ggplot(data=trainset,aes(sample=y2))+ geom_qq_line() + geom_qq()
#qplot(trainset$age,geom="density")
#trainset$residual=modelo_1_ajustar_y$residuals

qplot(x=trainset$age,y=trainset$children,geom="boxplot")
qplot(modelo_1_ajustar_y$residuals,geom="density")


qplot(y=trainset$age,x=trainset$charges,color=trainset$bmi)  + scale_color_continuous(type = "viridis")

trainset$children1=trainset$bmi %>% cut(c(0,18.5,25,30,100))

qplot(y=trainset$age,x=trainset$charges,color=trainset$children)

qplot(y=trainset$age,x=trainset$charges,color=trainset$bmi1)

ggplot(data=trainset,aes(x=y2,color=smoker))+geom_density()

trainset[349,]
busqueda=trainset %>% filter( smoker=="yes")

modelo_bruto=lm(data=trainset,y2~smoker+age+sex+children+region+bmi)

MASS::stepAIC(modelo_bruto,scope = list(upper=~smoker*age*sex*children*region*bmi,lower=~1),direction = "both")

autoplot(lm(formula = y2 ~ smoker + age + sex + children + region + bmi + 
    smoker:age + smoker:bmi + smoker:children + age:children + 
    region:bmi + age:region + smoker:sex + age:sex, data = trainset))

```

Ojo! Las poblaciones de fumadores y no fumadores son muy diferentes! Hacer dos modelos para cada 1
