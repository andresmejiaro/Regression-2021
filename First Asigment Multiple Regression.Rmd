---
title: "Multiple Regression"
author: "Ignacio Almodovar & Andres Mejia"
date: "11/23/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(magrittr)
library(ggplot2)
library(glmnet)
library(psych)
library(ggfortify)
library(caret)
library(stringr)
setwd("Datasets_Chapters_1_to_4")
```

## 1. Show that the properties of least squares estimators are satisfied using the following definitions:

$$
\begin{matrix}
\hat{\beta}=(X'X)^{-1}X'Y\\
\hat Y=X(X'X)^{-1}X'Y=HY\\
\hat\epsilon=Y-\hat Y=(I-H)Y
\end{matrix}
$$

### The residuals are orthogonal to the predictors.
### The sum of the residuals is zero.

This property is equivalent to $x_i^t \hat \epsilon=0$ or in matrix form $X'\hat \epsilon =0$, this also includes the second result due to the fact that $X$ contains a column of ones in order to account for the intercept term. To see why this is true consider.

$$
X'\hat \epsilon=X'(I-H)Y=X'(I-X(X'X)^{-1}X')Y=(X'-X'X(X'X)^{-1}X')Y=(X'-X')Y=0
$$

### The sum of the observed data is equal to the sum of the fitted values:

To prove this we will use the following descomposition of the $Y$ vector.


$$
Y=HY+(I-H)Y=\hat Y +\hat \epsilon
$$
Then multiplying with a column of ones and using the ortogonality of the ones with the error term

$$
\begin{matrix}
1^tY=1^t\hat  Y +1^t \hat \epsilon\\
\sum_{i=1}^n Y_i= \sum_{i=1}^n \hat Y_i +0=\sum_{i=1}^n \hat Y_i
\end{matrix}
$$


### The residuals are orthogonal to the fitted values.

Let's calculate the product
 
 $$
( \hat Y )'\hat \epsilon =(HY)'(I-H)Y=Y'H(I-H)Y=Y'(H-H)Y=Y'0Y=0
 $$
Here we used the fact that $H$ is symmetric and idempotent ($H^2=H$).

# 2. Using model modall, check numerically that the properties of the least squares estimates are satisfied.

### The sum of the residuals is 0 

$$\sum^{n}_{i=1}=\hat{\epsilon_i}$$
```{r}
bodyfat <- read.table(file = "Datasets_Chapters_1_to_4/bodyfat.txt", header = TRUE)
modall <- lm(hwfat ~., data = bodyfat)
sum(modall$residuals)
```

### The sum of the observed data is equal to the sum of the fitted values

$$\sum^n_{i=1}Y_i=\sum^n_{i=1}\hat{Y}_i$$

```{r}
sum(bodyfat$hwfat)
sum(modall$fitted.values)
```

### The residuals are orthogonal to the preditors

$$\sum_{i=1}^n x_{i}\hat{\epsilon}_i$$

```{r}
bodymatrix=as.matrix(select(bodyfat,-hwfat))
sum(t(bodymatrix)%*%modall$residuals)
```


### The residuals are orthogonal to the fitted values

$$\sum_{i=1}^n \hat{y_{i}}\hat{\epsilon}_i$$

```{r}
sum(modall$fitted.values*modall$residuals)
```

# 3.Check that for the dataset index.txt, the least squares es- timates of the parameters are: $\hat\beta_{0}=4.267$ and $\hat\beta_{1}= 1.373$, using the results in section 2.4.1 (not using the lm() function). 


$$\hat{\beta_0}=\overline Y - \hat{\beta_1}\overline X$$
$$\hat{\beta_1}=\frac{S_{xy}}{S^2_x}$$

Using R we can easily compute all those values.

```{r}
indice=read.table(file="Datasets_Chapters_1_to_4/index.txt", header = TRUE)
hatx=mean(indice$PovPct)
haty=mean(indice$Brth15to17)
varx=var(indice$PovPct)
samcov=cov(indice$PovPct,indice$Brth15to17)
b1=samcov/varx
b0=haty-b1*hatx
cat("b0=",b0,"b1=",b1)
```


#4  Given the response variable y and the covariates x2 and x3 in the dataset Transform V2.txt dataset , check if is necessary to transform any variable and the residual graphs to show that the transformed model is correct.

```{r}
TransformV2=readr::read_table("Datasets_Chapters_1_to_4/Transform_V2.txt")
```

First of all we plot a panel where we can see and compare the different distributions that our predictors follow, as well as the correlations between them. 

```{r}
pairs.panels(TransformV2, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
```

We can see that the response "y" is very correlated to both "x2" and "x3". Therefore, the response could be expressed by just one of the values. Nevertheless we are first going to fit a model using both predictors and see how it behavies.

```{r}
autoplot(lm(data=TransformV2,y~x2+x3))
```

In the first plot we can see that the variance between the Residuals and the Fitted values is not constant at all

In order to fix that, we can apply a transformation to our variables. First of all we are going to try to transform both of them using the square. 

```{r}
autoplot(lm(data=TransformV2,y~x2+x2^2+x3+x3^2))
```

As we have not obtained the results that we wanted we are going to try a different transformation.

As we saw before, in the first panel of plots we saw that the response Y was very correlated to both "x2" and "x3". Therefore, we are going to try a new model using just the square root transformation of just one of the variables.

```{r}
m4=lm(data=TransformV2,y~x3+I(x3^(1/2)))
summary(m4)
autoplot(m4)

```

Within this last transformation we can see that we have achieve the linear regression assumptions for a model, and we can say that it is good enough to be used. The variance is constant in all the trace of the plot, and it fits well the qnorm line. 

<!-- Bullet points -->

<!-- 1. Observamos que en las graficas de scale location y fitted tenemos una linea aproximadamente rectas con puntos distribuidos de forma homogenea tanto en la parte superior como inferior de la linea. -->

<!-- 2. Observamos que los residuales parecen distribuirse de manera normal viendo la grÃ¡fica qq. -->

<!-- 3. Ningun punto tiene una distacia de cook pronblematica, es decir, mayor a 0.5. -->


# 5.Given the response variable y and the covariates x1 and x2 in the datset Transform2 V2.txt, check if is necessary to transform any variable and the residual graphs to show that the transformed model is correct.

Again, for this exercise we are also going to plot a first panel to show how our data behaves.

```{r}
Transform2_V2=readr::read_table("Transform2_V2.txt")

pairs.panels(Transform2_V2, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )

modelo0=lm(data=Transform2_V2,y2~x1+x2)
autoplot(modelo0)
```
Again we can see that the model used does not fit the normality assumptions, so we are again going to transform the data. However, in this case, instead of transforming the predictors it is better to transform the response variable using the X method.


```{r}
MASS::boxcox(data=Transform2_V2,y2~x1+x2)
Transform2_V2 %<>% mutate(y2=(y2^-1-1)*-1)

pairs.panels(Transform2_V2, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
```

Once we have our response normalized, we can compute our new model using again just one of the predictors, as both of them are very correlated between them and with the response variable.

```{r}
m2=lm(data=Transform2_V2,y2~x1)
summary(m2)
autoplot(m2)
```

# 6. In the case of ridge regression, calculate bias $\hat\beta$ and show that $Var(\hat\beta)\geq Var(\hat\beta)$

We can write the bias of the ridge estimator as:

$$\hat\beta_\lambda=(X^tX+\lambda I)^{-1}X^{t}Y$$
As we know that $Y=(X\beta+\epsilon)$, then:

$$\hat\beta_\lambda=(X^tX+\lambda I)^{-1}X^{t}(X\beta+\epsilon)=(X^tX+\lambda I)^{-1}X^{t}X\beta + (X^tX+\lambda I)^{-1}X^t\epsilon$$

Therefore, the expected value of $\hat\beta_\lambda$:

$$
\begin{matrix}
E[\hat\beta_\lambda]=(X^tX+\lambda I)^{-1}X^{t}X\beta + (X^tX+\lambda I)^{-1}X^tE[\epsilon]=\\(X^tX+\lambda I)^{-1}X^{t}X\beta*0=\\ (X^tX+\lambda I)^{-1}X^{t}X\beta
\end{matrix}
$$
With this result we can say that the ridge estimator is unbiased if and only if $(X^tX+\lambda I)^{-1}X^{t}X=I$. This is only possible if $\lambda=0$ and that is when the ridge estimator coincides with the OLS estimator. Therefore, the bias of the estimator is:

$$
\begin{matrix}
E[\beta_{\lambda}]-\beta=  (X^tX+\lambda I)^{-1}X^{t}X\beta = (X^tX+\lambda I)^{-1}X^{t}X\beta-(X^tX)^{-1}X^tX\beta=\\ [(X^tX+\lambda I)^{-1}-(X^tX)^{-1}]X^tX\beta
\end{matrix}$$

To show that $Var[\hat\beta_{OLS}]\geq Var[\hat\beta_{ridge}]$:

We know that $Var[\hat\beta]=\sigma^2(X^tX)^{-1}$. Then,

$$
\begin{matrix}
Var[\hat\beta_\lambda]=\sigma^2(X^tX+\lambda I)^{-1}X^tX(X^tX+\lambda I)^{-1}=\\ \sigma^2(X^tX+\lambda I)^{-1}X^tX(X^tX)^{-1}X^tX(X^tX+\lambda I)^{-1}
\end{matrix}
$$
Defining M as $M=X^tX(X^tX+\lambda I)^{-1}$, the difference of variances is:

$$
\begin{matrix}
Var[\hat\beta_{OLS}] - Var[\hat\beta_{ridge}]  =  \sigma^2(X^tX)^{-1} - \sigma^2M^t(X^tX)^{-1}M=\\ \sigma^2[M^t(M^t)^{-1}(X^tX)^{-1}M^{-1}M - M^t(X^tX)^{-1}M]=\\
\sigma^2M^t[(M^t)^{-1}(X^tX)^{-1}M^{-1}-(X^tX)^{-1}]M=\\
\sigma^2M^t[(X^tX)^{-1}(X^tX+\lambda)(X^tX)^{-1}(X^tX+\lambda)(X^tX)^{-1}-(X^tX)^{-1} ]M=\\
\sigma^2M^t[(I+\lambda(X^tX)^{-1})(X^tX)^{-1}(I+\lambda(X^tX)^{-1})-(X^tX)^{-1}]M=\\
\sigma^2M^t[((X^tX)^{-1}+\lambda(X^tX)^{-2})(I+\lambda(X^tX)^{-1})-(X^tX)^{-1}]M=\\
\sigma^2 M^t[(X^tX)^{-1}+\lambda (X^tX)^{-2}+\lambda(X^tX)^{-2}+\lambda^2(X^tX)^{-3}-(X^tX)^{-1}]M=\\
\sigma^2 M^t[2\lambda(X^tX)^{-2}+\lambda^2(X^tX)^{-3}]M=\\
\sigma^2(X^tX+\lambda I)^{-1}X^tX[2\lambda(X^tX)^{-2}+\lambda^2(X^tX)^{-3}]X^tX(X^tX+\lambda I)^{-1}=\\
\sigma^2(X^tX+\lambda I)^{-1}[2\lambda I+\lambda^2(X^tX)^{-1}](X^tX+\lambda I)^{-1}
\end{matrix}
$$
If $\lambda>0$, the latter matrix is positive definite because for any $v\neq 0$, we have:

$$z=(X^tX+\lambda I)^{-1}v\neq0$$
Therefore, we finally have:

$$
\begin{matrix}
v^T(Var[\hat\beta_{OLS}] - Var[\hat\beta_{ridge}])v=\sigma^2z^t(2\lambda I+\lambda^2(X^tX)^{-1})z=\sigma^2\lambda z^tz+\sigma^2\lambda^2x^t(X^tX)^{-1}z>0
\end{matrix}
$$
Because $X^tX$ and its inverse are positive definite.

# 7. Calculate the value of R2 and Ra2 for model fit.ridge and compare them with the results of modall (modall <- lm(hwfat ., data = bodyfat))

We will use the code used in class in order to create and predict the ridge regression model.

```{r}
bodyfat=read.table("Datasets_Chapters_1_to_4/bodyfat.txt")
#Calculate the regression matrix without the intercept 
X=model.matrix(hwfat ~.-1,data=bodyfat)  
fit.ridge=glmnet(X,bodyfat$hwfat,alpha=0) 
cv.out = cv.glmnet(X, bodyfat$hwfat, alpha = 0)  #alpha=0 means Ridge Regression  

opt_lambda <- cv.out$lambda.min 

#Fitting the model for that value of the ridge parameter 
predict(fit.ridge, type = "coefficients", s = opt_lambda)

predichos=predict(fit.ridge, newx=X, s = opt_lambda)
```

Now, we will compute the sums of squares and calculate de R squared and adjusted R squared.

```{r}
SST=sum((bodyfat$hwfat-mean(bodyfat$hwfat))^2)
SSM=sum((predichos-mean(bodyfat$hwfat))^2)
SSE=sum((predichos-bodyfat$hwfat)^2)

r2=1-SSE/SST

r2adj=1-(1-r2)*(78-1)/(78-6-1)

cat("R2=",r2,"R2adj=",r2adj)
```

From the slides, we get that in the original model $R^2=0.8918$ and $R^2_{adj}=0.8827$. As we can see, these values are slightly higher than the ridge model.

# 8. The dataset insurance.csv contains data of insurance premiums paid by people in USA depending on their personal characteristics.

## a) Find the model that gives the best prediction

First of all, we are going to split our data into two sets, training and testing, with a 70% of the data in training. Within this division, we can build the model on the training data set, then apply the to the test data to predict the outcome of new unseen observations.

Finally we can quantify the prediction error between the observed and the predicted outcome values.

```{r}
insurance=readr::read_csv("Datasets_Chapters_1_to_4/insurance.csv")
insurance$children %<>% as.factor()
index=createDataPartition(insurance$charges,p=0.7)
trainset=insurance[index$Resample1,]
testset=insurance[-index$Resample1,]
```

Once we have the data partition on which we are going to build our model, we have to normalize the data so, in this case, w our response variable "charges" using boxcox, so that the response is normalized.

```{r}
t=MASS::boxcox(data=testset,charges~1)
indicemax=which.max(t$y)
lambda=t$x[indicemax]

trainset$y2=forecast::BoxCox(trainset$charges,lambda)
testset$y2=forecast::BoxCox(testset$charges,lambda)
```

Now we will train the first model, we will use the predictors that we might have a strong relationship with the response variable. Therefore, we are going to use the predictors: age, sex and two interactions, bmi with smokers as we think that bmi is only a risk factor when a person smokes and sex with number of children as we think that the market behaves differently with men and women regard to premium of each child.

```{r}
modelo_0_logico=lm(data=trainset,y2~age+bmi:smoker+sex+sex:children)
summary(modelo_0_logico)
autoplot(modelo_0_logico)
```

From this model we notice that in the residuals vs fitted there are three "zones". Coloring based on bmi and smoker we the following information, on which we are going to base the rest of the model. 

```{r}
trainset$residuales=modelo_0_logico$residuals
trainset$fit1=modelo_0_logico$fitted.values

ggplot(trainset,aes(x=residuales,y=fit1,color=smoker)) + geom_point()
ggplot(trainset,aes(x=residuales,y=fit1,color=bmi)) + 
  geom_point() + scale_color_continuous(type = "viridis")

trainset$highbmi=trainset$bmi<30
testset$highbmi=testset$bmi>30

trainset$bmi1=trainset$bmi %>% cut(c(0,20,30,100))
testset$bmi1=testset$bmi %>% cut(c(0,20,30,100))

ggplot(trainset,aes(x=residuales,y=fit1,color=bmi1)) + 
  geom_point() + scale_color_discrete(type = "viridis")
```

Using these predictors in the model.

```{r}
modelo_1_ajustar_y=lm(data=trainset,y2~age:bmi1+smoker:bmi+region+children)
summary(modelo_1_ajustar_y)
autoplot(modelo_1_ajustar_y)

```
We notice that when added the variable age, the predictive power of the model increases significantly, however the structure of the residuals is significantly worse. We suspect this is due to colinearity, we will use Ridge regression to see if any improvements can be made.

```{r}
#Calculate the regression matrix without the intercept
X=model.matrix(y2~age:bmi1+smoker:bmi+region+children-1,data=trainset)
X2=model.matrix(y2~age:bmi1+smoker:bmi+region+children-1,data=testset)

fit.ridge=glmnet(X,trainset$y2,alpha=0)
cv.out = cv.glmnet(X, trainset$y2, alpha = 0)
opt_lambda <- cv.out$lambda.min 

#Fitting the model for that value of the ridge parameter
modelo_final=predict(fit.ridge, type = "coefficients", s = opt_lambda)
predicted=predict(fit.ridge,newx = X2, s = opt_lambda)

residuos=predicted-testset$y2
ggplot(data=testset,aes(sample=residuos))+ geom_qq_line() + geom_qq(color="lightblue")
qplot(y=residuos,x=testset$y2)
```

Let's calculate the $R^2$ in the testset for the Ridge regression.

```{r}
SST=sum((testset$y2-mean(testset$y2))^2)
SSM=sum((predicted-mean(testset$y2))^2)
SSE=sum((predicted-testset$y2)^2)

r2=1-SSE/SST

r2adj=1-(1-r2)*(400-1)/(400-9)

cat("R2=",r2,"R2adj=",r2adj)
```

Let's calculate the $R^2$ in the testset for the Linear regression.

```{r}
predict_lineal=predict(modelo_1_ajustar_y,newdata=testset)

SST=sum((testset$y2-mean(testset$y2))^2)
SSM=sum((predict_lineal-mean(testset$y2))^2)
SSE=sum((predict_lineal-testset$y2)^2)

r2=1-SSE/SST

r2adj=1-(1-r2)*(400-1)/(400-9)

cat("R2=",r2,"R2adj=",r2adj)
```

As we can see the model does not get better with the Ridge regression. Therefore we will stick with the linear model that we created.

## b) What is the profile of the people that pay more (or less) for their insurance?

The people that are likely to pay more are the ones that have more than 4 children, smokers with a high bmi and generally, the premium increases with the age.


